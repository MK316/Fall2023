{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNtrxOMdTpad0I6ifCJhVHh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MK316/Fall2023/blob/main/Pdf_to_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. PDF to text"
      ],
      "metadata": {
        "id": "UlP481EXDpqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "id": "RFq8zkq4BSmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ File to upload from your computer"
      ],
      "metadata": {
        "id": "LpN2cFAKfn4g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3w4Z29zBLdG"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Convert pdf => text\n",
        "+ Save text as 'mytext.txt'\n",
        "\n"
      ],
      "metadata": {
        "id": "b2Jlj3exfLhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        # Create a PDF reader\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        text = \"\"\n",
        "\n",
        "        # Loop through all the pages and extract text\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "\n",
        "    return text\n",
        "\n",
        "pdf_filename = list(uploaded.keys())[0]  # gets the name of the first uploaded file\n",
        "pdf_text = extract_text_from_pdf(pdf_filename)\n",
        "\n",
        "# Save the extracted text to \"mytext.txt\" in the Colab folder\n",
        "with open(\"mytext.txt\", \"w\") as file:\n",
        "    file.write(pdf_text)\n",
        "\n",
        "len(pdf_text)\n"
      ],
      "metadata": {
        "id": "r2JHtcSwBPKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Frequency using **NLTK**"
      ],
      "metadata": {
        "id": "hjpPXcf9Dfvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Install and import packages"
      ],
      "metadata": {
        "id": "Yjft9HonfwuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.text import Text\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "OYWb2ge7DwaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Read text 'mytext.txt'"
      ],
      "metadata": {
        "id": "kwzMVQrHf4ta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('mytext.txt', 'r') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "7AJa5ACeD0JC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ The following regular expression ^[a-zA-Z]+$ ensures that only tokens with one or more alphabetical characters are included, effectively excluding punctuations, numbers, and other non-words."
      ],
      "metadata": {
        "id": "uUEMOgyYFKL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = nltk.word_tokenize(text)\n",
        "\n",
        "# Remove punctuation, numbers, non-words, and English stop words\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stop_words and re.match('^[a-zA-Z]+$', token)]\n"
      ],
      "metadata": {
        "id": "3xR3w26rFHle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Frequency list to csv file 'frequency_table.csv'"
      ],
      "metadata": {
        "id": "Whx6oS1AgEqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate frequency distribution\n",
        "fdist = FreqDist(filtered_tokens)\n",
        "\n",
        "# Convert frequency distribution to dataframe and save to CSV\n",
        "freq_df = pd.DataFrame(fdist.items(), columns=['Word', 'Frequency'])\n",
        "freq_df.to_csv('frequency_table.csv', index=False)\n"
      ],
      "metadata": {
        "id": "rYkug3d6D_CH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Concordance with a keyword (user input)"
      ],
      "metadata": {
        "id": "JJPiR3B1gKdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keyword = input(\"Enter the keyword: \")\n",
        "\n",
        "# Create a Text object\n",
        "text_obj = Text(tokens)\n",
        "\n",
        "# Display concordance\n",
        "text_obj.concordance(keyword, width=80, lines=20)  # width=80 ensures we see roughly 10 words before and after the keyword\n"
      ],
      "metadata": {
        "id": "7DSVzpn8EBEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# [2] Frequency list with page number information"
      ],
      "metadata": {
        "id": "aLzZg2lBHKIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1] Installation"
      ],
      "metadata": {
        "id": "dngv-Y6CNl6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk PyPDF2\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "import PyPDF2\n",
        "from nltk.corpus import stopwords\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')  # for POS tagging\n",
        "nltk.download('words')  # New addition for checking English words\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import words  # New addition"
      ],
      "metadata": {
        "id": "Xe22EjwEHOdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload PDF\n",
        "uploaded = files.upload()\n",
        "pdf_filename = list(uploaded.keys())[0]"
      ],
      "metadata": {
        "id": "2pgKiEP0M4Zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2] Extract Text from PDF and Track Pages\n",
        "\n",
        "Now we'll extract text from the PDF and track on which page(s) a token appears."
      ],
      "metadata": {
        "id": "dmTbtwluHZuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_with_page_numbers(pdf_path):\n",
        "    with open(pdf_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "\n",
        "        text_per_page = []\n",
        "        for page_num, page in enumerate(reader.pages, start=1):  # Starting from 1 to match with typical PDF pagination\n",
        "            adjusted_page_num = page_num - 11  # Adjusting the page number\n",
        "            if adjusted_page_num > 0:  # We only consider pages after the content begins\n",
        "                text_per_page.append((adjusted_page_num, page.extract_text()))\n",
        "\n",
        "    return text_per_page"
      ],
      "metadata": {
        "id": "BclKIAjUHRWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3] Process Text: Remove Stopwords, Punctuations, Single Characters, and Proper Nouns"
      ],
      "metadata": {
        "id": "7apa_SypHbc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_words = set(words.words())  # Create a set of English words"
      ],
      "metadata": {
        "id": "y69UIo2mNxR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_with_pages = extract_text_with_page_numbers(pdf_filename)\n",
        "\n",
        "all_tokens_with_pages = []\n",
        "for page_num, text in text_with_pages:\n",
        "    all_tokens_with_pages.extend(process_text(text, page_num))\n",
        "\n",
        "# Group by tokens and collate page numbers\n",
        "token_to_pages = {}\n",
        "for token, page_num in all_tokens_with_pages:\n",
        "    if token not in token_to_pages:\n",
        "        token_to_pages[token] = set()\n",
        "    token_to_pages[token].add(page_num)\n",
        "\n",
        "# Convert to frequency list\n",
        "freq_list = [(token, len(pages), ','.join(map(str, sorted(list(pages))))) for token, pages in token_to_pages.items()]\n",
        "\n",
        "freq_list.sort(key=lambda x: x[1], reverse=True)  # sort by frequency\n"
      ],
      "metadata": {
        "id": "eQjhmTEfHflK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Remove proper nouns, nonwords, words with less than 2 letters"
      ],
      "metadata": {
        "id": "YGXKtdjBgbYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text(text, page_num):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens_lower = [token.lower() for token in tokens]  # Convert all tokens to lowercase right away\n",
        "\n",
        "    tagged = nltk.pos_tag(tokens_lower)  # Tag the lowercase tokens\n",
        "\n",
        "    filtered_tokens = [token for token, tag in zip(tokens_lower, tagged)\n",
        "                       if token not in stop_words\n",
        "                       and re.match('^[a-zA-Z]+$', token)\n",
        "                       and len(token) > 2\n",
        "                       and token in english_words  # Check if it's an English word\n",
        "                       and tag not in ['NNP', 'NNPS']]\n",
        "\n",
        "    return [(token, page_num) for token in filtered_tokens]\n"
      ],
      "metadata": {
        "id": "YhWeElMBN1s2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Write the list as csv 'freq.csv'"
      ],
      "metadata": {
        "id": "nIECQL7Jgidw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(freq_list, columns=[\"Token\", \"Frequency\", \"Page Numbers\"])\n",
        "df.to_csv('freq.csv', index=False)"
      ],
      "metadata": {
        "id": "g0q2DIG8OTxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Download csv file"
      ],
      "metadata": {
        "id": "yI8_2dbQgnbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files.download('freq.csv')"
      ],
      "metadata": {
        "id": "qRtrKsIaOV07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Word list by character length"
      ],
      "metadata": {
        "id": "qSw_k6F7JK9q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1] Display words with length condition"
      ],
      "metadata": {
        "id": "KwlFhHRhhWg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wordlist = filtered_tokens\n",
        "len(wordlist)"
      ],
      "metadata": {
        "id": "SCDH1hGQV9-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desired_length = int(input(\"Enter the desired token length: \"))\n",
        "\n",
        "matching_tokens = [token for token in tokens if len(token) == desired_length]\n",
        "\n",
        "print(f\"Tokens of length {desired_length}:\")\n",
        "print(matching_tokens)\n"
      ],
      "metadata": {
        "id": "sUMDeZGlV9cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2] Longest to shortest: word frequency list"
      ],
      "metadata": {
        "id": "kp8z_0bvWdek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_with_pages = extract_text_with_page_numbers(pdf_filename)\n",
        "\n",
        "all_tokens_with_pages = []\n",
        "for page_num, text in text_with_pages:\n",
        "    all_tokens_with_pages.extend(process_text(text, page_num))\n",
        "\n",
        "# Group by tokens and collate page numbers\n",
        "token_to_pages = {}\n",
        "for token, page_num in all_tokens_with_pages:\n",
        "    if token not in token_to_pages:\n",
        "        token_to_pages[token] = set()\n",
        "    token_to_pages[token].add(page_num)\n",
        "\n",
        "# Convert to frequency list\n",
        "freq_list = [(token, len(pages), sorted(list(pages))) for token, pages in token_to_pages.items()]\n",
        "\n",
        "# Sort by token length\n",
        "freq_list.sort(key=lambda x: len(x[0]), reverse=True)\n"
      ],
      "metadata": {
        "id": "wnCUN3DqWiSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from google.colab import files\n",
        "\n",
        "# Save the freq_list to a CSV file\n",
        "with open('freq_list.csv', 'w', newline='') as csvfile:\n",
        "    csv_writer = csv.writer(csvfile)\n",
        "\n",
        "    # Write the header\n",
        "    csv_writer.writerow([\"Token\", \"Number of Letters\", \"Frequency\", \"Page Numbers\"])\n",
        "\n",
        "    # Write the data\n",
        "    for row in freq_list:\n",
        "        token, freq, page_nums = row\n",
        "        num_letters = len(token)  # Calculate the number of letters for each token\n",
        "        csv_writer.writerow([token, num_letters, freq, ', '.join(map(str, page_nums))])\n",
        "\n",
        "# If you want to download the file to your computer\n",
        "files.download('freq_list.csv')\n"
      ],
      "metadata": {
        "id": "elTIEO7vW1x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the file to your computer\n",
        "files.download('freq_list.csv')\n"
      ],
      "metadata": {
        "id": "af0UtTj1XGbo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}